{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafc425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake # For Keyword Extraction\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration # For Text Summarization\n",
    "#For aspect based sentimental analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab707d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Keyword Extraction\n",
    "def extract_keywords(text, language=\"en\", max_keywords=10):\n",
    "    # Create a YAKE keyword extractor\n",
    "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_keywords, dedupLim=0.7, dedupFunc='seqm')\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    # Deduplicate keywords using a set and combining similar keywords\n",
    "    unique_keywords = set()\n",
    "    deduplicated_keywords = []\n",
    "    for keyword, score in keywords:\n",
    "        # Check for similar keywords already in the set\n",
    "        similar_keywords = [kw for kw in unique_keywords if kw in keyword or keyword in kw]\n",
    "        \n",
    "        if not similar_keywords:\n",
    "            deduplicated_keywords.append(keyword)\n",
    "            unique_keywords.add(keyword)\n",
    "\n",
    "    # Return the list of deduplicated keywords\n",
    "    return deduplicated_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda06201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Summarization \n",
    "# Loading pre-trained model and tokenizer\n",
    "\n",
    "def text_summarization(product_review):\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenizing and generating summary\n",
    "    inputs = tokenizer.encode(\"summarize: \" + product_review, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=50, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9976042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:27:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f472318e63f44e43a68602f4a60f526f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d36fe012fca47c78bbc9438a9c79dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.7.0/models/lemma/combined_nocharlm.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:27:46 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus                   |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-05-04 15:27:46 INFO: Using device: cpu\n",
      "2024-05-04 15:27:46 INFO: Loading: tokenize\n",
      "2024-05-04 15:27:46 INFO: Loading: mwt\n",
      "2024-05-04 15:27:46 INFO: Loading: pos\n",
      "2024-05-04 15:27:46 INFO: Loading: lemma\n",
      "2024-05-04 15:27:46 INFO: Loading: constituency\n",
      "2024-05-04 15:27:47 INFO: Loading: depparse\n",
      "2024-05-04 15:27:47 INFO: Loading: sentiment\n",
      "2024-05-04 15:27:47 INFO: Loading: ner\n",
      "2024-05-04 15:27:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Here instead of narrowsing the sentiment of aspects into positive or negative,\n",
    "#we are returning the sentiment word and leaving the choice to the user itself for judging the sentiment of the aspect.\n",
    "#( Clearly shown in the output)  \n",
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "    \n",
    "    txt = txt.lower()  # Lowercasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt)  # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        txt_list = nltk.word_tokenize(line)  # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list)  # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        doc = nlp(line)  # Object of Stanza NLP Pipeline\n",
    "        \n",
    "        # Getting the dependency relations between the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "        # Converting it into an appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = txt_list[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "        featureList = []\n",
    "        for i in taggedList:\n",
    "            if i[1].startswith('JJ'):  # Filter adjectives\n",
    "                featureList.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if ((j[0] == i or j[1] == i) and j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]):\n",
    "                    if j[0] == i:\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i, filist])\n",
    "\n",
    "    for i in fcluster:\n",
    "        aspect = i[0]\n",
    "        related_adjectives = ' '.join(i[1]).replace(' ', '')  # Combine words and remove spaces\n",
    "\n",
    "        # Check for negation and adjust sentiment\n",
    "        if \"not\" in i[1]:\n",
    "            aspect_tokens = txt.split()\n",
    "            not_index = i[1].index(\"not\")\n",
    "            if not_index < len(aspect_tokens) - 1:\n",
    "                next_word = aspect_tokens[not_index + 1]\n",
    "                aspect = \"not_\" + aspect if next_word in stop_words else \"not \" + aspect\n",
    "            related_adjectives = related_adjectives.replace(\"not\", \"\")\n",
    "\n",
    "        if aspect not in dic:\n",
    "            dic[aspect] = related_adjectives\n",
    "        else:\n",
    "            dic[aspect] += ' ' + related_adjectives\n",
    "            \n",
    "    finalcluster = [[value, [key]] for key, value in dic.items()]\n",
    "    return finalcluster\n",
    "\n",
    "nlp = stanza.Pipeline()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e154558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be softcoded so that user can enter their specific required review below given is sample review by our group \n",
    "Product_Review = \"\"\"\n",
    "The new smartphone is fantastic. The camera quality is excellent, capturing sharp and clear photos. \n",
    "The battery life exceeds expectations, lasting all day with regular use. \n",
    "The sleek design and vibrant display make it a pleasure to use. However,it has high price.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b2a98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      " \n",
      "The new smartphone is fantastic. The camera quality is excellent, capturing sharp and clear photos. \n",
      "The battery life exceeds expectations, lasting all day with regular use. \n",
      "The sleek design and vibrant display make it a pleasure to use. However,it has high price.\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " The camera quality is excellent, capturing sharp and clear photos. The battery life exceeds expectations, lasting all day with regular use. The sleek design and vibrant display make it a pleasure to use.\n",
      "\n",
      "Keywords: ['smartphone is fantastic', 'capturing sharp and clear photos', 'camera quality is excellent', 'battery life exceeds expectations', 'lasting all day with regular', 'sleek design and vibrant display make it a pleasure', 'However,it has high price']\n",
      "\n",
      "[['smartphone', ['new']], ['smartphone', ['fantastic']], ['quality', ['excellent']], ['photos', ['sharp']], ['', ['clear']], ['use', ['regular']], ['design', ['sleek']], ['display', ['vibrant']], ['price', ['high']]]\n"
     ]
    }
   ],
   "source": [
    "#Original_Review\n",
    "print(\"Original Review:\\n\", Product_Review)\n",
    "summary = text_summarization(Product_Review)\n",
    "#Summary\n",
    "print(\"\\nGenerated Summary:\\n\", summary)\n",
    "# Extracting deduplicated keywords\n",
    "keywords = extract_keywords(Product_Review)\n",
    "print()\n",
    "# Printing the keywords\n",
    "print(\"Keywords:\", keywords)\n",
    "# Printing the aspects \n",
    "print()\n",
    "print(aspect_sentiment_analysis(Product_Review, stop_words, nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177720a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce1b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354ab0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6aa05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
