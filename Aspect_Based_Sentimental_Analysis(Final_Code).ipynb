{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d899e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:19:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f11a3d7c654bcb9a33773bf31b5446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:19:51 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus                   |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2023-12-10 23:19:51 INFO: Using device: cpu\n",
      "2023-12-10 23:19:51 INFO: Loading: tokenize\n",
      "2023-12-10 23:19:51 INFO: Loading: mwt\n",
      "2023-12-10 23:19:51 INFO: Loading: pos\n",
      "2023-12-10 23:19:51 INFO: Loading: lemma\n",
      "2023-12-10 23:19:51 INFO: Loading: constituency\n",
      "2023-12-10 23:19:51 INFO: Loading: depparse\n",
      "2023-12-10 23:19:51 INFO: Loading: sentiment\n",
      "2023-12-10 23:19:51 INFO: Loading: ner\n",
      "2023-12-10 23:19:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['smartphone', ['new']], ['smartphone', ['fantastic']], ['quality', ['excellent']], ['photos', ['sharp']], ['', ['clear']], ['use', ['regular']], ['design', ['sleek']], ['display', ['vibrant']], ['price', ['high']]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "\n",
    "# Here instead of narrowsing the sentiment of aspects into positive or negative,\n",
    "#we are returning the sentiment word and leaving the choice to the user itself for judging the sentiment of the aspect.\n",
    "#( Clearly shown in the output)  \n",
    "def aspect_sentiment_analysis(txt, stop_words, nlp):\n",
    "    \n",
    "    txt = txt.lower()  # Lowercasing the given Text\n",
    "    sentList = nltk.sent_tokenize(txt)  # Splitting the text into sentences\n",
    "\n",
    "    fcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        txt_list = nltk.word_tokenize(line)  # Splitting up into words\n",
    "        taggedList = nltk.pos_tag(txt_list)  # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        doc = nlp(line)  # Object of Stanza NLP Pipeline\n",
    "        \n",
    "        # Getting the dependency relations between the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "\n",
    "        # Converting it into an appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = txt_list[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "        featureList = []\n",
    "        for i in taggedList:\n",
    "            if i[1].startswith('JJ'):  # Filter adjectives\n",
    "                featureList.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if ((j[0] == i or j[1] == i) and j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"]):\n",
    "                    if j[0] == i:\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i, filist])\n",
    "\n",
    "    for i in fcluster:\n",
    "        aspect = i[0]\n",
    "        related_adjectives = ' '.join(i[1]).replace(' ', '')  # Combine words and remove spaces\n",
    "\n",
    "        # Check for negation and adjust sentiment\n",
    "        if \"not\" in i[1]:\n",
    "            aspect_tokens = txt.split()\n",
    "            not_index = i[1].index(\"not\")\n",
    "            if not_index < len(aspect_tokens) - 1:\n",
    "                next_word = aspect_tokens[not_index + 1]\n",
    "                aspect = \"not_\" + aspect if next_word in stop_words else \"not \" + aspect\n",
    "            related_adjectives = related_adjectives.replace(\"not\", \"\")\n",
    "\n",
    "        if aspect not in dic:\n",
    "            dic[aspect] = related_adjectives\n",
    "        else:\n",
    "            dic[aspect] += ' ' + related_adjectives\n",
    "            \n",
    "    finalcluster = [[value, [key]] for key, value in dic.items()]\n",
    "    return finalcluster\n",
    "\n",
    "Product_Review = \"\"\"\n",
    "The new smartphone is fantastic. The camera quality is excellent, capturing sharp and clear photos. \n",
    "The battery life exceeds expectations, lasting all day with regular use. \n",
    "The sleek design and vibrant display make it a pleasure to use. However,it has high price.\n",
    "\"\"\"\n",
    "\n",
    "nlp = stanza.Pipeline()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(aspect_sentiment_analysis(Product_Review, stop_words, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a7c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
